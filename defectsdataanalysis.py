# -*- coding: utf-8 -*-
"""defectsDataAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AYYzt4p79Dw7xJCMjEF0Tn6G1i8feVJI

# **>> METADATA**

This is a PROMISE data set made publicly available in order to encourage repeatable, verifiable, refutable, and/or improvable predictive models of software engineering.

If you publish material based on PROMISE data sets then, please follow the acknowledgment guidelines posted on the PROMISE repository.

 web page http://promise.site.uottawa.ca/SERepository .


  ### **Attribute Information:**
 	
      1. loc             : numeric % McCabe's line count of code
      2. v(g)            : numeric % McCabe "cyclomatic complexity"
      3. ev(g)           : numeric % McCabe "essential complexity"
      4. iv(g)           : numeric % McCabe "design complexity"
      5. n               : numeric % Halstead total operators + operands
      6. v               : numeric % Halstead "volume"
      7. l               : numeric % Halstead "program length"
      8. d               : numeric % Halstead "difficulty"
      9. i               : numeric % Halstead "intelligence"
     10. e               : numeric % Halstead "effort"
     11. b               : numeric % Halstead 
     12. t               : numeric % Halstead's time estimator
     13. lOCode          : numeric % Halstead's line count
     14. lOComment       : numeric % Halstead's count of lines of comments
     15. lOBlank         : numeric % Halstead's count of blank lines
     16. lOCodeAndComment: numeric
     17. uniq_Op         : numeric % unique operators
     18. uniq_Opnd       : numeric % unique operands
     19. total_Op        : numeric % total operators
     20. total_Opnd      : numeric % total operands
     21: branchCount     : numeric % of the flow graph
     22. defects         : {false,true} % module has/has not one or more reported defects
 
 >> **Missing attributes:** None

 >> **Class Distribution:** the class value (defects) is discrete 
    false: 2106 = 19.35%
    true:  8779 = 80.65%

<img align = "center" src="https://vit.ac.in/ICBRAMR/ICBRAMR_website/vit.png" alt="Girl in a jacket" width="500" height="600">

<h1 align = "center" style = "color: #608bef">Software Defect Prediction System</h1>

<p></p>
<p></p>
<h2 align = "center" style = "color: #a82222">SYED AYAZ IMAM (18BCE0660)</h2>
<h2 align = "center" style = "color: #a82222">AISHIKA SAHA (18BCE2168)</h2>

# **>> IMPORTING THE DATASET FROM GOOGLE DRIVE**
"""

!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zJSUeg63pbdwnDgw2TJ7ZRPDIhoJAeNK' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1zJSUeg63pbdwnDgw2TJ7ZRPDIhoJAeNK" -O data.csv && rm -rf /tmp/cookies.txt

"""# **>> IMPORTING LIBRARIES**"""

import numpy as np # linear algebra
import pandas as pd # SV file I/O
import matplotlib.pyplot as plt # data visualization
import seaborn as sns # statistical data visualization
from sklearn import model_selection
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import tensorflow as tf

data = pd.read_csv('data.csv')

data.head()

defect_true_false = data.groupby('defects')['b'].apply(lambda x: x.count())
print('False: ',defect_true_false[0])
print('True: ',defect_true_false[1])

data = data.dropna()
data.isnull().sum()

"""# **>> DATA ANALYSIS**"""

print("The shape of the data is: ", data.shape)

#Simple Statistics
data.describe()

f,ax = plt.subplots(figsize = (15, 15))
sns.heatmap(data.corr(), annot = True, linewidths = .5, fmt = '.2f')
plt.show()

"""## **>> DATA PREPROCESSING**

<img align = "center" src = "https://media.geeksforgeeks.org/wp-content/uploads/min-max-normalisation.jpg"></img>
"""

from sklearn import preprocessing

scale_v = data[['v']]
scale_b = data[['b']]

minmax_scaler = preprocessing.MinMaxScaler()

v_scaled = minmax_scaler.fit_transform(scale_v)
b_scaled = minmax_scaler.fit_transform(scale_b)

data['v_ScaledUp'] = pd.DataFrame(v_scaled)
data['b_ScaledUp'] = pd.DataFrame(b_scaled)

data

data.dtypes

data['branchCount'] = pd.to_numeric(data['branchCount'],errors = 'coerce')
data['total_Opnd'] = pd.to_numeric(data['total_Opnd'],errors = 'coerce')
data['total_Op'] = pd.to_numeric(data['total_Op'],errors = 'coerce')
data['uniq_Opnd'] = pd.to_numeric(data['uniq_Opnd'],errors = 'coerce')
data['uniq_Op'] = pd.to_numeric(data['uniq_Op'],errors = 'coerce')

data.shape

def values(x):
    if x == True:
        return 1
    return 0

data['defects'] = data['defects'].map(values)

X = data.iloc[:,0:21]
Y = data.iloc[:,21]

Y = Y.map(values)

X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.2, random_state=3)

print("X train Shape: ", X_train.shape)
print("X test Shape: ", X_test.shape)
print("Y train Shape: ", Y_train.shape)
print("Y test Shape: ", Y_test.shape)

"""<img align = "center" src = "https://i.stack.imgur.com/SPq4w.png"></img>"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""<img align = "center" src = "https://i.ibb.co/9sCQkXq/Screenshot-2021-05-19-at-10-45-42-PM.png"></img>"""

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units = 512, input_dim=21, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 512, input_dim=21, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 256, input_dim=21, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))
model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))


opt = Adam(lr=1e-5)

model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])

model.summary()

model.fit(X_train, Y_train, batch_size = 128, epochs =100)